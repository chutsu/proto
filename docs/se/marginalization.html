<h1>Marginalization</h1>

<p>As a reminder, marginalization is about having a joint density $p(x, y)$
over two variables $x$ and $y$, and we would like to marginalize out or
"eliminate a variable", lets say $y$ in this case:
<equation>
  p(x) = \int_{y} p(x, y)
</equation>
resulting in a density $p(x)$ over the remaining variable $x$.</p>

<p>Now, if the density was in covariance form with mean $\boldsymbol{\mu}$ and
covariance $\mathbf{\Sigma}$, partitioned as follows:
<equation>
  p(x, y) = \mathcal{N}(
    % Mean
    \begin{bmatrix}
      \boldsymbol\mu_{x} \\ 
      \boldsymbol\mu_{y}
    \end{bmatrix},
    % Covariance
    \begin{bmatrix}
      \mathbf\Sigma_{xx}, \mathbf\Sigma_{xy} \\ 
      \mathbf\Sigma_{yx}, \mathbf\Sigma_{yy}
    \end{bmatrix}
  )
</equation>
marginalization is simple, as the corresponding sub-block
$\mathbf{\Sigma}_{xx}$ already contains the covariance on $x$ i.e.,
<equation>
  p(x) = \mathcal{N}(
    % Mean
    \boldsymbol\mu_{x},
    % Covariance
      \mathbf\Sigma_{xx}
  ).
</equation>
In the nonlinear-least squares formulation, however, we can only obtain the
covariance $\mathbf{\Sigma}$ with the following property,
<equation>
  \mathbf{\Sigma} = \Mat{H}^{-1}
</equation>

where $\Mat{H}$ is the hessian matrix in a Gauss-Newton system. It just so
happens that when we're trying to solve $\Mat{H}\delta{\Vec{x}} = \Vec{b}$ for
$\delta{\Vec{x}}$, when we apply the Schur's complement two things occur:

<ol>
  <li>invert $\Mat{H}$ to solve for $\delta{\Vec{x}}$</li>
  <li>Marginalize out old states</li>
</ol>

The marginalized $\mathbf{H}$ (after Schur's complement) will be equivalent to
$\mathbf{\Sigma}_{xx}^{-1}.$</p>


<h2>Using Shur's Complement for marginalization</h2>

<p>In a Gauss-Newton system,
<equation>
  \Mat{H} \delta\state = \Vec{b} ,
</equation>
it so happens that Schur's complement can be used to both invert and
marginalize out the old states. First let $\state_\mu$ be the states to be
marginalized out, $\state_{\lambda}$ be the set of states related to those by
error terms, and $\state_{\rho}$ be the set of remaining states. Partitioning
the Hessian, error state and R.H.S of the Gauss-Newton system gives:
<equation>
  \begin{bmatrix}
    \Mat{H}_{\mu\mu} & \Mat{H}_{\mu\lambda_{1}} \\
    \Mat{H}_{\lambda_{1}\mu} & \Mat{H}_{\lambda_{1}\lambda_{1}}
  \end{bmatrix}
  \begin{bmatrix}
    \delta\state_{\mu} \\
    \delta\state_{\lambda}
  \end{bmatrix}
  =
  \begin{bmatrix}
    \Vec{b}_{\mu} \\
    \Vec{b}_{\lambda}
  \end{bmatrix}
</equation>
and applying the Shur complement operation yields:
<equation>
  \Mat{H}^{\ast}_{\lambda_{1}\lambda_{1}}
  &=
  \Mat{H}_{\lambda_{1}\lambda_{1}} -
  \Mat{H}_{\lambda_{1}\mu}
  \Mat{H}_{\mu\mu}^{-1}
  \Mat{H}_{\mu\lambda_{1}}
  \\
  \Vec{b}^{\ast}_{\lambda_{1}}
  &=
  \Vec{b}_{\lambda_{1}} -
  \Mat{H}_{\lambda_{1}\mu}
  \Mat{H}_{\mu\mu}^{-1}
  \Vec{b}_{\mu}
</equation>
where $\Vec{b}^{\ast}_{\lambda_{1}}$ and
$\Mat{H}^{\ast}_{\lambda_{1}\lambda_{1}}$ are non-linear functions of
$\state_\lambda$ and $\state_\mu$.</p>

<p>The finite deviation $\Delta{\chi}= \Phi^{-1}(\log(\bar{\state} \boxplus
\state_{0}^{-1}))$ represents state updates that occur after marginalization,
where $\bar{\state}$ is our current estimate for $\state$. Introducing and
approximating the R.H.S of the Gauss-Newton equation with $\Delta{\chi}$ and
the first order Taylor series results in,
<equation>
  <!-- \label{eq:gn_rhs_v2} -->
  \Vec{b} + \dfrac{\delta{b}}{\delta{\Delta{\chi}}} \bigg\rvert_{\state_{0}}
    = \Vec{b} - \Mat{H} \Delta{\chi}.
</equation>
Partioning the matrix $\Mat{H}$ and $\Vec{b}$ into $\mu$ and $\lambda$,
<equation>
  <!-- \label{eq:gn_rhs_v2_partitioned} -->
  \begin{bmatrix}
    \Vec{b}_{\mu} \\ \Vec{b}_{\lambda_{1}}
  \end{bmatrix}
    =
    \begin{bmatrix}
      \Vec{b}_{\mu, 0} \\ \Vec{b}_{\lambda_{1}, 0}
    \end{bmatrix}
    -
    \begin{bmatrix}
      \Mat{H}_{\mu\mu} & \Mat{H}_{\mu\lambda_{1}} \\
      \Mat{H}_{\lambda_{1}\mu} & \Mat{H}_{\lambda_{1}\lambda_{1}}
    \end{bmatrix}
    \begin{bmatrix}
      \Delta{\chi}_{\mu} \\
      \Delta{\chi}_{\lambda_{1}}
    \end{bmatrix}.
</equation>
Substituting in the above  to the R.H.S of the Gauss-Newton system, $\Mat{H}
\delta{\state} = \Vec{b}$, results in,
<equation>
  \Vec{b}^{\ast}_{\lambda_{1}} =
    \underbrace{
      \Vec{b}_{\lambda_{1}, 0} -
      \Mat{H}_{\lambda_{1}\mu}
      \Mat{H}_{\mu\mu}^{-1}
      \Vec{b}_{\mu, 0}
    }_{\Vec{b}^{\ast}_{\lambda_{1}, 0}}
    -
    \Mat{H}^{\ast}_{\lambda_{1}\lambda_{1}}
    \Delta{\chi}_{\lambda_{1}}.
</equation>
</p>


<h3>Derivation of Schur's Complement for Marginalization</h3>

<p>
From the Gauss-Newton system, $\mathbf{H} \mathbf{x} = \mathbf{b}$, we can
derive the marginalization of the old states in $\mathbf{x}$ algebraically.
Let us decompose the system as:
<equation>
  % H
  \begin{bmatrix}
    \mathbf{H}_{11} & \mathbf{H}_{12} \\ 
    \mathbf{H}_{21} & \mathbf{H}_{22}
  \end{bmatrix}
  % x
  \begin{bmatrix}
    \mathbf{x}_{1} \\ 
    \mathbf{x}_{2}
  \end{bmatrix}
  =
  % b
  \begin{bmatrix}
    \mathbf{b}_{1} \\ 
    \mathbf{b}_{2}
  \end{bmatrix}
</equation>
If we multiply out the block matrices and vectors out we get:
<equation>
  % Line 1
  \mathbf{H}_{11} \mathbf{x}_{1} + \mathbf{H}_{12} \mathbf{x}_{2}
    = \mathbf{b}_{1} \\
  % Line 2
  \mathbf{H}_{21} \mathbf{x}_{1} + \mathbf{H}_{22} \mathbf{x}_{2}
    = \mathbf{b}_{2}
</equation>
Now if we want to marginalize out the $\mathbf{x}_{2}$, we simply rearrange the
second equation above to be w.r.t. $\mathbf{x}_{2}$ like so:
<equation>
  % Line 1
  \mathbf{H}_{21} \mathbf{x}_{1} + \mathbf{H}_{22} \mathbf{x}_{2}
    &= \mathbf{b}_{2} \\
  % Line 2
  \mathbf{H}_{22} \mathbf{x}_{2}
    &= \mathbf{b}_{2} - \mathbf{H}_{21} \mathbf{x}_{1} \\
  % Line 3
  \mathbf{x}_{2}
    &= \mathbf{H}_{22}^{-1} \mathbf{b}_{2}
		- \mathbf{H}_{22}^{-1} \mathbf{H}_{21} \mathbf{x}_{1} \\
</equation>
substitute our $\mathbf{x}_{2}$ above back into $\mathbf{H}_{11} \mathbf{x}_{1}
+ \mathbf{H}_{12} \mathbf{x}_{2} = \mathbf{b}_{1}$, and rearrange the terms so
it is w.r.t $\mathbf{x}_{1}$ to get:
<equation>
  % Line 1
  \mathbf{H}_{11} \mathbf{x}_{1} + \mathbf{H}_{12}
  (\mathbf{H}_{22}^{-1} \mathbf{b}_{2}
    - \mathbf{H}_{22}^{-1} \mathbf{H}_{21} \mathbf{x}_{1})
  &= \mathbf{b}_{1} \\
  % Line 2
  \mathbf{H}_{11} \mathbf{x}_{1}
  + \mathbf{H}_{12} \mathbf{H}_{22}^{-1} \mathbf{b}_{2}
  - \mathbf{H}_{12} \mathbf{H}_{22}^{-1} \mathbf{H}_{21} \mathbf{x}_{1}
  &= \mathbf{b}_{1} \\
  % Line 3
  (\mathbf{H}_{11}
		- \mathbf{H}_{12}\mathbf{H}_{22}^{-1}\mathbf{H}_{21}) \mathbf{x}_{1}
  &= \mathbf{b}_{1} - \mathbf{H}_{12} \mathbf{H}_{22}^{-1} \mathbf{b}_{2}
</equation>
thus the Schur Complement of $\mathbf{H}_{22}$ in $\mathbf{H}$ is:
<equation>
  \mathbf{H} / \mathbf{H}_{22} :=
    \mathbf{H}_{11}
    - \mathbf{H}_{12}\mathbf{H}_{22}^{-1}\mathbf{H}_{21} \\
  \mathbf{b} / \mathbf{b}_{2} :=
    \mathbf{b}_{1} - \mathbf{H}_{12} \mathbf{H}_{22}^{-1} \mathbf{b}_{2}
</equation>
If you want to marginalize out $\mathbf{x}_{1}$ you can follow the same process
above but w.r.t $\mathbf{x}_{1}$.
</p>

<hr>

<p>Let us consider the following scenario. A state vector, $\state$, during the
time interval $[0, k]$ will contain $m$ old states to be marginalized out and
$r$ remain states which we wish to keep. i.e. $\state =
[\state_{m}^{\transpose} \quad \state_{r}^{\transpose}]^{\transpose}$. Then the
cost function, $c(\cdot)$, can be written as a function of $\state$ at time $k$
as,
<equation>
  c(\state_{k}) &= c(\state_{m}, \state_{r}) \\
                &= c(\state_{m}) + c(\state_{r}).
</equation>
The intuition behind the above is since the state at time $k$ can be
partitioned into $m$ and $r$, the cost can also be decomposed. Utilizing this
property, the multivariate optimization can also be decomposed as follows,
<equation>

  \min_{\state_{m}, \state_{r}} c(\state_{m}, \state_{r})
    &= \min_{\state_{r}} (\min_{\state_{m}} c(\state_{m}, \state_{r})) \\
    &= \min_{\state_{r}} (c(\state_{r}) + \min_{\state_{m}} c(\state_{m})) .

</equation>
The equation above shows the minimization problem can be solved by first
optimizing for the states $\state_{m}$, and then forming a prior towards the
problem of solving for $\state_{r}$. The reformulation of the minimization
problem entails no approximation.</p>
