<h1>Marginalization</h1>

<p>As a reminder, marginalization is about having a joint density $p(x, y)$
over two variables $x$ and $y$, and we would like to marginalize out or
"eliminate a variable", lets say $y$ in this case:
<equation>
  p(x) = \int_{y} p(x, y)
</equation>
resulting in a density $p(x)$ over the remaining variable $x$.</p>

<p>Now, if the density was in covariance form with mean $\boldsymbol{\mu}$ and
covariance $\mathbf{\Sigma}$, partitioned as follows:
<equation>
  p(x, y) = \mathcal{N}(
    % Mean
    \begin{bmatrix}
      \boldsymbol\mu_{x} \\ 
      \boldsymbol\mu_{y}
    \end{bmatrix},
    % Covariance
    \begin{bmatrix}
      \mathbf\Sigma_{xx}, \mathbf\Sigma_{xy} \\ 
      \mathbf\Sigma_{yx}, \mathbf\Sigma_{yy}
    \end{bmatrix}
  )
</equation>
marginalization is simple, as the corresponding sub-block
$\mathbf{\Sigma}_{xx}$ already contains the covariance on $x$ i.e.,
<equation>
  p(x) = \mathcal{N}(
    % Mean
    \boldsymbol\mu_{x},
    % Covariance
      \mathbf\Sigma_{xx}
  ).
</equation>
</p>

<p>In the nonlinear-least squares case, however, we took the negative-log on
the multi-variate Gaussian distribution to arrive at the nonlinear least
squares cost function of the form:
<equation>
  \mathcal{N}(\mathbf{z}, \boldsymbol{\mu}, \mathbf{\Sigma}) &= 
    \dfrac{1}{\sqrt{|2\pi\mathbf{\Sigma}|}}
    \exp(
      -\dfrac{1}{2} 
      || \mathbf{z} - \mathbf{h}(\boldsymbol{\mu})||^{2}_{\Sigma}
    )\\
  J &= \min_{x} ||\mathbf{e} ||^{2}_{\Sigma} \\
  J &= \min_{x} ||\mathbf{\Sigma}^{-1/2} \mathbf{e} ||^{2}_{2}
</equation>

<equation>
  \mathbf{\Sigma} = \mathbf{H}^{-1}
</equation>

It just so happens that when we're trying to invert the Hessian sub matrix
block via Schur's complement, we are at the same time performing
marginalization.


<h2>Shur's Complement</h2>

<p>Let $\Mat{M}$ be a matrix that consists of block matrices $\Mat{A}$, $\Mat{B}$,
$\Mat{C}$, $\Mat{D}$,
<equation>
  \Mat{M} =
  \begin{bmatrix}
    \Mat{A} & \Mat{B} \\
    \Mat{C} & \Mat{D}
  \end{bmatrix}
</equation>
if $\Mat{A}$ is invertible, the Schur's complement of the block $\Mat{A}$ of the
matrix $\Mat{B}$ is the defined by
<equation>
  \Mat{M}/\Mat{A} = \Mat{D} - \Mat{C} \Mat{A}^{-1} \Mat{B}.
</equation>
<equation>
  \Mat{M}/\Mat{D} = \Mat{A} - \Mat{B} \Mat{D}^{-1} \Mat{C}.
</equation>
</p>


---

<h2>Using Shur's Complement for marginalization</h2>

<p>In a Gauss-Newton system,
<equation>
  \Mat{H} \delta\state = \Vec{b} ,
</equation>
it so happens that Schur's complement can be used to both invert and
marginalize out the old states. First let $\state_\mu$ be the states to be
marginalized out, $\state_{\lambda}$ be the set of states related to those by
error terms, and $\state_{\rho}$ be the set of remaining states. Partitioning
the Hessian, error state and R.H.S of the Gauss-Newton system gives:
<equation>
  \begin{bmatrix}
    \Mat{H}_{\mu\mu} & \Mat{H}_{\mu\lambda_{1}} \\
    \Mat{H}_{\lambda_{1}\mu} & \Mat{H}_{\lambda_{1}\lambda_{1}}
  \end{bmatrix}
  \begin{bmatrix}
    \delta\state_{\mu} \\
    \delta\state_{\lambda}
  \end{bmatrix}
  =
  \begin{bmatrix}
    \Vec{b}_{\mu} \\
    \Vec{b}_{\lambda}
  \end{bmatrix}
</equation>
and applying the Shur complement operation yields:
<equation>
  \Mat{H}^{\ast}_{\lambda_{1}\lambda_{1}}
  &=
  \Mat{H}_{\lambda_{1}\lambda_{1}} -
  \Mat{H}_{\lambda_{1}\mu}
  \Mat{H}_{\mu\mu}^{-1}
  \Mat{H}_{\mu\lambda_{1}}
  \\
  \Vec{b}^{\ast}_{\lambda_{1}}
  &=
  \Vec{b}_{\lambda_{1}} -
  \Mat{H}_{\lambda_{1}\mu}
  \Mat{H}_{\mu\mu}^{-1}
  \Vec{b}_{\mu}
</equation>
where $\Vec{b}^{\ast}_{\lambda_{1}}$ and
$\Mat{H}^{\ast}_{\lambda_{1}\lambda_{1}}$ are non-linear functions of
$\state_\lambda$ and $\state_\mu$.</p>

<p>The finite deviation $\Delta{\chi}= \Phi^{-1}(\log(\bar{\state} \boxplus
\state_{0}^{-1}))$ represents state updates that occur after marginalization,
where $\bar{\state}$ is our current estimate for $\state$. Introducing and
approximating the R.H.S of the Gauss-Newton equation with $\Delta{\chi}$ and
the first order Taylor series results in,
<equation>
  <!-- \label{eq:gn_rhs_v2} -->
  \Vec{b} + \dfrac{\delta{b}}{\delta{\Delta{\chi}}} \bigg\rvert_{\state_{0}}
    = \Vec{b} - \Mat{H} \Delta{\chi}.
</equation>
Partioning \eqref{eq:gn_rhs_v2} into $\mu$ and $\lambda$,
<equation>
  <!-- \label{eq:gn_rhs_v2_partitioned} -->
  \begin{bmatrix}
    \Vec{b}_{\mu} \\ \Vec{b}_{\lambda_{1}}
  \end{bmatrix}
    =
    \begin{bmatrix}
      \Vec{b}_{\mu, 0} \\ \Vec{b}_{\lambda_{1}, 0}
    \end{bmatrix}
    -
    \begin{bmatrix}
      \Mat{H}_{\mu\mu} & \Mat{H}_{\mu\lambda_{1}} \\
      \Mat{H}_{\lambda_{1}\mu} & \Mat{H}_{\lambda_{1}\lambda_{1}}
    \end{bmatrix}
    \begin{bmatrix}
      \Delta{\chi}_{\mu} \\
      \Delta{\chi}_{\lambda_{1}}
    \end{bmatrix}.
</equation>
Substituting in the above  to the R.H.S of the Gauss-Newton system, $\Mat{H}
\delta{\state} = \Vec{b}$, results in,
<equation>
  \Vec{b}^{\ast}_{\lambda_{1}} =
    \underbrace{
      \Vec{b}_{\lambda_{1}, 0} -
      \Mat{H}_{\lambda_{1}\mu}
      \Mat{H}_{\mu\mu}^{-1}
      \Vec{b}_{\mu, 0}
    }_{\Vec{b}^{\ast}_{\lambda_{1}, 0}}
    -
    \Mat{H}^{\ast}_{\lambda_{1}\lambda_{1}}
    \Delta{\chi}_{\lambda_{1}}.
</equation>
</p>


<h3>Derivation of Schur's Complement for Marginalization</h3>

<p>
From the Gauss-Newton system, $\mathbf{H} \mathbf{x} = \mathbf{b}$, we can
derive the marginalization of the old states in $\mathbf{x}$ algebraically.
Let us decompose the system as:
<equation>
  % H
  \begin{bmatrix}
    \mathbf{H}_{11} & \mathbf{H}_{12} \\ 
    \mathbf{H}_{21} & \mathbf{H}_{22}
  \end{bmatrix}
  % x
  \begin{bmatrix}
    \mathbf{x}_{1} \\ 
    \mathbf{x}_{2}
  \end{bmatrix}
  =
  % b
  \begin{bmatrix}
    \mathbf{b}_{1} \\ 
    \mathbf{b}_{2}
  \end{bmatrix}
</equation>
If we multiply out the block matrices and vectors out we get:
<equation>
  % Line 1
  \mathbf{H}_{11} \mathbf{x}_{1} + \mathbf{H}_{12} \mathbf{x}_{2}
    = \mathbf{b}_{1} \\
  % Line 2
  \mathbf{H}_{21} \mathbf{x}_{1} + \mathbf{H}_{22} \mathbf{x}_{2}
    = \mathbf{b}_{2}
</equation>
Now if we want to marginalize out the $\mathbf{x}_{2}$, we simply rearrange the
second equation above to be w.r.t. $\mathbf{x}_{2}$ like so:
<equation>
  % Line 1
  \mathbf{H}_{21} \mathbf{x}_{1} + \mathbf{H}_{22} \mathbf{x}_{2}
    &= \mathbf{b}_{2} \\
  % Line 2
  \mathbf{H}_{22} \mathbf{x}_{2}
    &= \mathbf{b}_{2} - \mathbf{H}_{21} \mathbf{x}_{1} \\
  % Line 3
  \mathbf{x}_{2}
    &= \mathbf{H}_{22}^{-1} \mathbf{b}_{2}
		- \mathbf{H}_{22}^{-1} \mathbf{H}_{21} \mathbf{x}_{1} \\
</equation>
substitute our $\mathbf{x}_{2}$ above back into $\mathbf{H}_{11} \mathbf{x}_{1}
+ \mathbf{H}_{12} \mathbf{x}_{2} = \mathbf{b}_{1}$, and rearrange the terms so
it is w.r.t $\mathbf{x}_{1}$ to get:
<equation>
  % Line 1
  \mathbf{H}_{11} \mathbf{x}_{1} + \mathbf{H}_{12}
  (\mathbf{H}_{22}^{-1} \mathbf{b}_{2}
    - \mathbf{H}_{22}^{-1} \mathbf{H}_{21} \mathbf{x}_{1})
  &= \mathbf{b}_{1} \\
  % Line 2
  \mathbf{H}_{11} \mathbf{x}_{1}
  + \mathbf{H}_{12} \mathbf{H}_{22}^{-1} \mathbf{b}_{2}
  - \mathbf{H}_{12} \mathbf{H}_{22}^{-1} \mathbf{H}_{21} \mathbf{x}_{1}
  &= \mathbf{b}_{1} \\
  % Line 3
  (\mathbf{H}_{11}
		- \mathbf{H}_{12}\mathbf{H}_{22}^{-1}\mathbf{H}_{21}) \mathbf{x}_{1}
  &= \mathbf{b}_{1} - \mathbf{H}_{12} \mathbf{H}_{22}^{-1} \mathbf{b}_{2}
</equation>
We end up with the Schur Complement of $\mathbf{H}_{22}$ in $\mathbf{H}$:
<equation>
  \mathbf{H} / \mathbf{H}_{22} :=
    \mathbf{H}_{11}
    - \mathbf{H}_{12}\mathbf{H}_{22}^{-1}\mathbf{H}_{21} \\
  \mathbf{b} / \mathbf{b}_{2} :=
    \mathbf{b}_{1} - \mathbf{H}_{12} \mathbf{H}_{22}^{-1} \mathbf{b}_{2}
</equation>
If you want to marginalize out $\mathbf{x}_{1}$ you can follow the same process
above but w.r.t $\mathbf{x}_{1}$, that is left as an exercise to the reader ;)
</p>

---

<p>Let us consider the following scenario. A state vector, $\state$, during the
time interval $[0, k]$ will contain $m$ old states to be marginalized out and
$r$ remain states which we wish to keep. i.e. $\state =
[\state_{m}^{\transpose} \quad \state_{r}^{\transpose}]^{\transpose}$. Then the
cost function, $c(\cdot)$, can be written as a function of $\state$ at time $k$
as,
<equation>
  c(\state_{k}) &= c(\state_{m}, \state_{r}) \\
                &= c(\state_{m}) + c(\state_{r}).
</equation>
The intuition behind \eqref{eq:ba_cost_fn} is since the state at time $k$ can
be partitioned into $m$ and $r$, the cost can also be decomposed. Utilizing
this property, the multi-variate optimization can also be decomposed as
follows,
<equation>

  \min_{\state_{m}, \state_{r}} c(\state_{m}, \state_{r})
    &= \min_{\state_{r}} (\min_{\state_{m}} c(\state_{m}, \state_{r})) \\
    &= \min_{\state_{r}} (c(\state_{r}) + \min_{\state_{m}} c(\state_{m})) .

</equation>
The equation above shows the minimization problem can be solved by first
optimizing for the states $\state_{m}$, and then forming a prior towards the
problem of solving for $\state_{r}$. The reformulation of the minimization
problem entails no approximation.</p>

<p>The Gauss Newton system for solving the minimization problem is,
<equation>
  \Mat{H} \Delta{\state} = \Vec{b}.
</equation>
Partitioning the $\Mat{H}$ matrix into $\mu$ for states to be marginalized out,
and $\lambda$ for states to remain,
<equation>
  \begin{bmatrix}
    \Mat{H}_{\mu\mu} & \Mat{H}_{\mu\lambda_{1}} \\
    \Mat{H}_{\lambda_{1}\mu} & \Mat{H}_{\lambda_{1}\lambda_{1}}
  \end{bmatrix}
  \begin{bmatrix}
    \delta\state_{\mu} \\
    \delta\state_{\lambda}
  \end{bmatrix}
  =
  \begin{bmatrix}
    \Vec{b}_{\mu} \\
    \Vec{b}_{\lambda}
  \end{bmatrix}
</equation>
Solving for $\Delta{\state}$ via the Schur's Complement operation,
<equation>
  \Mat{H}^{\ast}_{\lambda_{1}\lambda_{1}}
  &=
  \Mat{H}_{\lambda_{1}\lambda_{1}} -
  \Mat{H}_{\lambda_{1}\mu}
  \Mat{H}_{\mu\mu}^{-1}
  \Mat{H}_{\mu\lambda_{1}}
  \\
  \Vec{b}^{\ast}_{\lambda_{1}}
  &=
  \Vec{b}_{\lambda_{1}} -
  \Mat{H}_{\lambda_{1}\mu}
  \Mat{H}_{\mu\mu}^{-1}
  \Vec{b}_{\mu}
</equation>
where $\Vec{b}^{\ast}_{\lambda_{1}}$ and
$\Mat{H}^{\ast}_{\lambda_{1}\lambda_{1}}$ are non-linear functions of
$\state_\lambda$ and $\state_\mu$.</p>
