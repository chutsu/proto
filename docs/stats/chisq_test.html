<h1>Pearson's Chi-Squared Test</h1>

<p>Pearson's chi-squared test $\chi^2$ is a statistical test applied to sets of
<i>categorial data</i> to quantify the likelihood that the observed difference
between measured and predicted arose by chance. The test is used to assess
three types of comparisons:
<ul>
  <li><b>Goodness of fit</b>: checks whether the observed frequency
  distribution matches the theoretical distribution.</li>
  <li><b>Homogeneity</b>: compares the distribution of counts for two or more
  groups using the same categorical variable.</li>
  <li><b>Independence</b>: checks whether the unparied observations on two
  variables, expressed in a contingency table, are independent of each
  other.</li>
</ul>
</p>


<h3>Goodness of Fit</h3>

<p>
<equation>
  \chi^2 &= \sum^{n}_{i=1} \dfrac{(O_i - E_i)^2}{E_i} \\
         &= N \sum^{n}_{i=1} \dfrac{(O_i / N - p_i)^2}{p_i}
</equation>
where $\chi^2$ is Pearson's cumulative test statistic, $O_i$ is the number of
observations of type $i$, $N$ is the total number of observations, $E_i = N
p_i$ is the expected (theoretical) count of type $i$, and $n$ is the number of
cells in the table.</p>

<p>Once the chi-squared test statistic is calculated, the $p$-value is obtained by
comparing the value of the statistic to a chi-squared distribution. The number
of degrees of freedom is equal to the number of cells $n$, minus the reduction
in degrees of freedom, $p$.</p>


<h3>Example: Fairness of dice</h3>

<p>A 6-sided die is thrown 60 times. The number of times it lands with 1, 2, 3,
4, 5 and 6 face up is 5, 8, 9, 8, 20 and 20, respectively. Is the die biased,
according to the Pearson's chi-squared test at a significance level of 95%,
and, or 99%?</p>

<p>In this example $n = 6$ as there are 6 possible outcomes, 1 to 6. The null
hypothesis is that the die is unbaised, hence each dice number is expected to
occur the same number of times, in this case, $60 / n = 10$. The outcomes can
be tabulated as follows:
<table id="chisq_test">
  <thead>
    <tr>
      <th>$i$</th>
      <th>$O_i$</th>
      <th>$E_i$</th>
      <th>$O_i - E_i$</th>
      <th>$(O_i - E_i)^2$</th>
      <th>$\dfrac{(O_i - E_i)^2}{E_i}$</th>
    </tr>
  </thead>
  <tr>
    <td>1</td><td>5</td><td>10</td><td>5</td><td>25</td><td>2.5</td>
  </tr>
  <tr>
    <td>2</td><td>8</td><td>10</td><td>2</td><td>4</td><td>0.4</td>
  </tr>
  <tr>
    <td>3</td><td>9</td><td>10</td><td>1</td><td>1</td><td>0.1</td>
  </tr>
  <tr>
    <td>4</td><td>8</td><td>10</td><td>2</td><td>4</td><td>0.4</td>
  </tr>
  <tr>
    <td>5</td><td>10</td><td>10</td><td>0</td><td>0</td><td>0</td>
  </tr>
  <tr>
    <td>6</td><td>20</td><td>10</td><td>10</td><td>100</td><td>10</td>
  </tr>
</table>
<style>
  #chisq_test {
    border-collapse: collapse;
    margin-left: auto;
    margin-right: auto;
  }

  #chisq_test th {
    padding-left: 10px;
    padding-right: 10px;
    padding-bottom: 10px;
  }

  #chisq_test td {
    text-align: center;
  }

  #chisq_test thead tr {
    border-bottom: 1px solid #000;
  }
</style>
</p>

<p>The number of degrees of freedom is $n - 1 = 5$. The Upper tail critical values
of chi-square distribution gives a critical value of 11.070 at 95%
significance level:
<!-- % -->
<!-- \begin{table}[h] -->
<!--   \center -->
<!--   \begin{tabular}{p{1.6cm} | ccccc} -->
<!--     Degrees of Freedom -->
<!--     &#38; \multicolumn{5}{c}{Probability less than the critical value} \\ -->
<!--     &#38; \textbf{0.90} -->
<!--     &#38; \textbf{0.95} -->
<!--     &#38; \textbf{0.975} -->
<!--     &#38; \textbf{0.99} -->
<!--     &#38; \textbf{0.999} \\ -->
<!--     \hline -->
<!--     5 &#38; 9.236 &#38; 11.070 &#38; 12.833 &#38; 15.086 &#38; 20.515 -->
<!--   \end{tabular} -->
<!-- \end{table} -->
As the chi-squared statistic of 13.4 exceeds this critical value, the null
hypothesis is rejected and conclude that the die is biased at 95% significance
level. At 99% significance level, the critical value is 15.086. As the
chi-squared statistic does not exceed it, the null hypothesis
is not rejected and thus conclude that there is insufficient evidence to show
that the die is biased at 99% significance level.</p>


<h3>Problems with Pearson's Chi-Squared Test~\cite{Andrae2010}</h3>

<ul>
  <li>The degrees of freedom can only be estimated for <i>linear
      models</i>. It is non-trivial or near impossible to estimate the degrees of
    freedom for a <i>non-linear model</i>.</li>
  <li>The value of chi-squared itself is subject to noise in the data, as
    such the value is uncertain.</li>
</ul>

Knowing the degrees of freedom of the model in question is required for
chi-squared test. For $N$ data points and $P$ parameters, a naive guess is that
the number of degrees of freedom is $N - P$. This, however, as will be
demonstrated is not always the case.

The chi-squared test statistic, $\chi^2$, for continuous data is defined as,
<equation>
  \chi^2 = \sum^{N}_{n = 1}
    \left( \dfrac{(y_n - f(x_n, \theta))}{\sigma_n} \right)^2
</equation>
which is equivalent to maximizing the liklihood function. For a linear model,
the chi-squared statistic, $\chi^2$ can be written in matrix form as,
<equation>
  \chi^2 =
    (\Vec{y} - \Mat{X} \boldsymbol{\theta})^{\transpose}
    \Mat{\Sigma}^{-1}
    (\Vec{y} - \Mat{X} \boldsymbol{\theta}) .
</equation>
Rearranging in terms of the model parameters, $\boldsymbol{\theta}$, gives,
<equation>
  \boldsymbol{\theta} =
    (\Mat{X}^{\transpose} \Mat{\Sigma}^{-1} \Mat{X})^{-1}
    \Mat{X}^{\transpose} \Mat{\Sigma}^{-1} \Vec{y} .
</equation>
Finally, the prediction, $\hat{\Vec{y}}$, of the measurements, $\Vec{y}$, can
be obtained by multiplying the design matrix, $\Mat{X}$, with the model
parameters, $\boldsymbol{\theta}$, and further simplified to give us,
<equation>
  \hat{\Vec{y}}
    &= \Mat{X} \boldsymbol{\theta} \\
    &= \Mat{X} (\Transpose{\Mat{X}} \Mat{\Sigma}^{-1} \Mat{X})^{-1}
       \Transpose{\Mat{X}} \Mat{\Sigma}^{-1} \Vec{y} \\
    &= \Mat{H} \Vec{y}
</equation>
where $\Mat{H}$ is an $N \times N$ matrix, sometimes called the "hat matrix"
because it translates the measurement data, $\Vec{y}$, into a model prediction,
$\hat{\Vec{y}}$. The number of <i>effecitve</i> model parameters,
$P_{\text{eff}}$, is then given by the trace of $\Mat{H}$,
<equation>
  P_{\text{eff}} = \Trace{\Mat{H}} = \sum^{N}_{n = 1} H_{nn} = \Rank{\Mat{X}} .
</equation>
which also equals the rank of the design matrix $\Mat{X}$. And so,
$P_{\text{eff}} \leq P$, where the equality holds if and only if the design
matrix $\Mat{X}$ has full rank. Consequently, for linear models the number of
degrees of freedom is,
<equation>
  K = N - P_{\text{eff}} \geq N - P
</equation>
For nonlinear models, the degrees of freedom is not as straight forward.


<h4>Example 1</h4>

Let us consider a nonlinear model with three free parameters, $A$, $B$, and $C$,
<equation>
  f(x) = A \cos(Bx + C) .
</equation>
If we are given a set of $N$ measurement $(x_n, y_n, \sigma_n)$ such that no
two data points have identical $x_n$, then the model $f(x)$ is capable of
fitting any such data set perfectly. The way this works is by increasing the
"frequency" $B$ such that $f(x)$ can change on arbitrarily short scales. As $f(x)$
provides a perfect fit in this case, $\chi^2$ is equal to zero for all possible
noise realizations of the data. Evidently, this three-parameter model has
infinite flexibility (if there are no priors) and $K = N - P$ is a poor estimate
of the number of degrees of freedom, which actually is $K = 0$.


<h4>Example 2</h4>

To build upon the first example, three additional model parameters, $D$, $E$
and $F$ are added,

<equation>
  f(x) = A \cos(Bx + C) + D \cos(Ex + F) .
</equation>

If the fit parameter $D$ becomes small such that $|D| \leq |A|$, the second
component cannot influence the fit anymore and the two model parameters $E$ and
$F$ are "lost". In simple words: This model may change its flexibility during
the fitting procedure.

Hence, for nonlinear models, K may not even be constant. Of course, these two
examples do not verify the claim that always $K \neq N - P$ for nonlinear
models.  However, acting as counter-examples, they clearly falsify the claim
that $K = N - P$ is always true for nonlinear models.
