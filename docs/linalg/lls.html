<h1>Linear Least Squares</h1>

Linear problems generally have the form
<equation>
  \Mat{A} \Vec{x} = \Vec{b}
</equation>
If $\Mat{A}$ is skinny (number of rows is larger than number of columns) the
problem is over constrained and there is no \textit{unique} solution. Instead,
the problem can be solved by minizming the squared error between $\Mat{A}
\Vec{x}$ and $\Vec{b}$. The linear least squares problem is then defined as,
<equation>
  \min_{\Vec{x}} || \Mat{A} \Vec{x} - \Vec{b} ||^{2}_{2} \enspace,
</equation>
where the goal is to find an \textit{approximate} solution.

The local minima can be found when the derivative of the squared error is zero.
First the squared error is expanded to give:
<equation>
\begin{aligned}
  & (\Mat{A} \Vec{x} - \Vec{b})^{\transpose}
    (\Mat{A} \Vec{x} - \Vec{b}) \\
  & (\Transpose{\Vec{x}} \Transpose{\Mat{A}} \Mat{A} \Vec{x}
    - 2 \Transpose{\Vec{b}} \Mat{A} \Vec{x}
    + \Transpose{\Vec{b}} \Vec{b})
\end{aligned}
</equation>
then by differentiating the expanded squared error with respect to $\Vec{x}$,
setting the derivative to zero, and rearranging the equation with respect to
$\Vec{x}$ gives the following:
<equation>
\begin{aligned}
  % Line 1
  2 \Transpose{\Vec{x}} \Transpose{\Mat{A}} \Mat{A}
    - 2 \Transpose{\Vec{b}} \Mat{A} &= 0 \\
  % Line 2
  \Transpose{\Vec{x}} \Transpose{\Mat{A}} \Mat{A}
    &= \Transpose{\Vec{b}} \Mat{A} \\
  % Line 3
  \Transpose{\Mat{A}} \Mat{A} \Vec{x}
    &= \Transpose{\Mat{A}} \Vec{b} \\
  % Line 4
  \Vec{x}
    &= \left( \Transpose{\Mat{A}} \Mat{A} \right)^{-1}
      \Transpose{\Mat{A}} \Vec{b} \\
  % Line 5
  \Vec{x}
    &= \Mat{A}^{\dagger} \Vec{b} \enspace,
\end{aligned}
</equation>
where $\left( \Transpose{\Mat{A}} \Mat{A} \right)^{-1} \Transpose{\Mat{A}}$ is
known as the pseudo inverse $\Mat{A}^{\dagger}$.
