<h1>Linear Least Squares</h1>

<p>Linear problems generally have the form
<equation>
  \Mat{A} \Vec{x} = \Vec{b}
</equation>
If $\Mat{A}$ is skinny (number of rows is larger than number of columns) the
problem is over constrained and there is no <i>unique</i> solution. Instead,
the problem can be solved by minizming the squared error between $\Mat{A}
\Vec{x}$ and $\Vec{b}$. The linear least squares problem is then defined as,
<equation>
  \min_{\Vec{x}} || \Mat{A} \Vec{x} - \Vec{b} ||^{2}_{2} \enspace,
</equation>
where the goal is to find an <i>approximate</i> solution.

The local minima can be found when the derivative of the squared error is zero.
First the squared error is expanded to give:
<equation>

  & (\Mat{A} \Vec{x} - \Vec{b})^{\transpose}
    (\Mat{A} \Vec{x} - \Vec{b}) \\
  & (\Transpose{\Vec{x}} \Transpose{\Mat{A}} \Mat{A} \Vec{x}
    - 2 \Transpose{\Vec{b}} \Mat{A} \Vec{x}
    + \Transpose{\Vec{b}} \Vec{b})

</equation>
then by differentiating the expanded squared error with respect to $\Vec{x}$,
setting the derivative to zero, and rearranging the equation with respect to
$\Vec{x}$ gives the following:
<equation>

  % Line 1
  2 \Transpose{\Vec{x}} \Transpose{\Mat{A}} \Mat{A}
    - 2 \Transpose{\Vec{b}} \Mat{A} &= 0 \\
  % Line 2
  \Transpose{\Vec{x}} \Transpose{\Mat{A}} \Mat{A}
    &= \Transpose{\Vec{b}} \Mat{A} \\
  % Line 3
  \Transpose{\Mat{A}} \Mat{A} \Vec{x}
    &= \Transpose{\Mat{A}} \Vec{b} \\
  % Line 4
  \Vec{x}
    &= \left( \Transpose{\Mat{A}} \Mat{A} \right)^{-1}
      \Transpose{\Mat{A}} \Vec{b} \\
  % Line 5
  \Vec{x}
    &= \Mat{A}^{\dagger} \Vec{b} \enspace,

</equation>
where $\left( \Transpose{\Mat{A}} \Mat{A} \right)^{-1} \Transpose{\Mat{A}}$ is
known as the pseudo inverse $\Mat{A}^{\dagger}$.
</p>


<hr><!------------------------------------------------------------------------!>


<h2>Forward Substitution</h2>

<equation>
  \Mat{U} \Vec{x} = \Vec{b}
</equation>

<equation>
\begin{bmatrix}
  u_{11} & 0 & \dots & 0 \\
  u_{21} & u_{22} & \dots & 0 \\
  \vdots & \vdots & \ddots & \vdots \\
  u_{m1} & u_{m2} & \dots & u_{mn}
\end{bmatrix}
\begin{bmatrix}
  x_{1} \\
  x_{2} \\
  \vdots \\
  x_{m}
\end{bmatrix}
=
\begin{bmatrix}
  b_{1} \\
  b_{2} \\
  \vdots \\
  b_{m}
\end{bmatrix}
</equation>
writing out the above,
<equation>
  &u_{11} x_{1} = b_{1} \\
  &u_{21} x_{1} + u_{22} x_{2} = b_{2} \\
  &u_{31} x_{1} + u_{32} x_{2} + u_{33} x_{3} = b_{3} \\
  &\qquad\qquad\qquad\vdots \\
  &u_{m,1} x_{1} + u_{m,2} x_{2} + \dots + u_{m,n} x_{n} = b_{n}
</equation>
and rearranging to solve for $\Vec{x}$,
<equation>
  x_{1} &= b_{1} / u_{11} \\
  x_{2} &= (b_{2} - u_{21} x_{1}) / u_{22} \\
  x_{3} &= (b_{3} - u_{31} x_{1} - u_{32} x_{2} ) / u_{33} \\
  &\qquad\qquad\qquad\qquad\qquad\vdots \\
  x_{m} &= (b_{m} - u_{m,1} x_{1} - u_{m,2} x_{2} - \dots
            - u_{m,m-1} x_{m-1} ) / u_{m,n}
</equation>
or more generally,
<equation>
  \boxed{
    x_{i} = \dfrac{b_{i} - \sum_{j=1}^{i-1} u_{ij} x_{i}}{u_{ii}}
    \quad
    \text{where} \; 1 \leq i \leq n
  }.
</equation>


<hr><!------------------------------------------------------------------------!>


<h2>Backward Substitution</h2>

<equation>
  \Mat{L} \Vec{x} = \Vec{b}
</equation>

<equation>
\begin{bmatrix}
  l_{11} & l_{12} & \dots & l_{1n} \\
  0 & l_{22} & \dots & l_{2n} \\
  \vdots & \vdots & \ddots & \vdots \\
  0 & 0 & \dots & l_{mn} \\
\end{bmatrix}
\begin{bmatrix}
  x_{1} \\
  x_{2} \\
  \vdots \\
  x_{m}
\end{bmatrix}
=
\begin{bmatrix}
  b_{1} \\
  b_{2} \\
  \vdots \\
  b_{m}
\end{bmatrix}
</equation>
writing out the above,
<equation>
  &l_{11} x_{1} + l_{12} x_{2} + \dots + l_{1n} x_{n} = b_{1} \\
  &l_{22} x_{2} + \dots + l_{2n} x_{n} = b_{2} \\
  &\qquad\qquad\qquad\vdots \\
  &l_{mn} x_{n} = b_{n}
</equation>
and rearranging to solve for $\Vec{x}$,
<equation>
  x_{1} &= (b_{1} - l_{12} x_{2} - \dots - l_{1n} x_{n}) / l_{11} \\
  x_{2} &= (b_{2} - l_{22} x_{3} - \dots - l_{2n} x_{n}) / l_{22} \\
  &\qquad\qquad\qquad\vdots \\
  x_{m} &= b_{m} / l_{mn}
</equation>
or more generally,
<equation>
  \boxed{
    x_{i} = \dfrac{b_{i} - \sum_{j=i+1}^{1} l_{ij} x_{i}}{l_{ii}}
    \quad
    \text{where} \; i = n, n - 1, \cdots, 1
  }.
</equation>


<hr><!------------------------------------------------------------------------!>

<h2>Invert Lower Triangular Matrix</h2>

<p>If we have a lower triangular matrix $\Mat{L}$ and our objective is to find
$\Mat{L}^{-1}$, we can use the property
<equation>
  \Mat{L}\Mat{L}^{-1} = \I,
</equation>
and use forward-substitution to solve for $\Mat{L}^{-1}$ column by column,
staring from the first column $j$.
<equation>
\begin{bmatrix}
  l_{11} & 0 & \dots & 0 \\
  l_{21} & l_{22} & \dots & 0 \\
  \vdots & \vdots & \ddots & \vdots \\
  l_{m1} & l_{m2} & \dots & l_{mn}
\end{bmatrix}
\begin{bmatrix}
  a_{1j} \\
  a_{2j} \\
  \vdots \\
  a_{mj}
\end{bmatrix}
=
\begin{bmatrix}
  1 \\
  0 \\
  \vdots \\
  0
\end{bmatrix}
</equation>

<equation>
  & l_{11} a_{1j} = 1 \\
  & l_{21} a_{1j} + l_{22} a_{2j} = 0 \\
  & \qquad\qquad\qquad\vdots \\
  & l_{m1} a_{1j} + l_{m2} a_{2j} + \dots + l_{mn} a_{mj} = 0
</equation>

<equation>
  & a_{1j} = 1 / l_{11} \\
  & a_{2j} = - l_{21} a_{1j} / l_{22} \\
  & \qquad\qquad\qquad\vdots \\
  & a_{mj} = (-l_{m1} a_{1j} - l_{m2} a_{2j} 
              - \dots - l_{m,j-1} a_{m-1,j}) / l_{mn}
</equation>

<equation>
  a_{ij} = \dfrac{I_{ij} - \sum_{j=i+1}^{1} l_{ij} a_{i}}{l_{ii}}
  \quad
  \text{where} \; i = n, n - 1, \cdots, 1
</equation>

</p>
