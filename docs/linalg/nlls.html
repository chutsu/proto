<h1>Non-linear Least Squares</h1>

<p>A non-linear least squares is an optimization problem that minimizes the sum
of squares of the residual functions, and has the form
<equation>
  \cost = \text{argmin}_{\Vec{x}} \enspace
    \dfrac{1}{2} \enspace
    \Vec{e}(\Vec{x})^{\transpose}
    \Mat{W} \,
    \Vec{e}(\Vec{x})
</equation>
where the error function, $\Vec{e}(\cdot)$, depends on the optimization
parameter, $\Vec{x} \in \real^{n}$. The error function, $\Vec{e}(\cdot)$, has a
form of
<equation>
  \Vec{e}_{i} =
    \Vec{z} - \Vec{h}(\Vec{x})
</equation>
is defined as the difference between the measured value, $\Vec{z}$, and the
estimated value calculated using the measurement function, $\Vec{h}(\cdot)$.
</p>

<p>A local minima for the problem is found when the gradient of the cost,
$\cost$, is zero
<equation>
  \dfrac{\partial{\cost}}{\partial{\Vec{x}}}
  &=
    \dfrac{\partial{\cost}}{\partial{\Vec{e}}}
    \dfrac{\partial{\Vec{e}}}{\partial{\Vec{x}}} \\
  &=
    \Vec{e}(\Vec{x})^{\transpose}
    \Mat{W}
    \dfrac{\partial{\Vec{e}}}{\partial{\Vec{x}}} \\
  &=
    \Vec{e}(\Vec{x})^{\transpose}
    \Mat{W}
    \Vec{E}(\Vec{x})
</equation>
linearizing $\Vec{e}(\Vec{x})$ with the first-order Taylor series,
$\Vec{e}(\Vec{x}) \approx \Vec{e}(\bar{\Vec{x}}) + \Vec{E}(\bar{\Vec{x}})
\Delta\Vec{x}$, gives,
<equation>
  \dfrac{\partial{\cost}}{\partial{\Vec{x}}}
  &=
    (\Vec{e}(\bar{\Vec{x}}) + \Vec{E}(\bar{\Vec{x}})\Delta\Vec{x})^{\transpose}
    \Mat{W} \Vec{E}(\Vec{x})
  = 0 \\
  &\Vec{e}(\bar{\Vec{x}})^{\transpose} \Mat{W} \Vec{E}(\bar{\Vec{x}})
    + \Delta\Vec{x}^{\transpose} \Vec{E}(\bar{\Vec{x}})^{\transpose}
      \Mat{W}
      \Vec{E}(\bar{\Vec{x}})
    = 0 \\
  &\Vec{E}(\bar{\Vec{x}})^{\transpose} \Mat{W} \, \Vec{e}(\bar{\Vec{x}})
    + \Vec{E}(\bar{\Vec{x}})^{\transpose}
      \Mat{W}
      \Vec{E}(\bar{\Vec{x}})
      \Delta\Vec{x}
    = 0 \\
  &\underbrace{
      \Vec{E}(\bar{\Vec{x}})^{\transpose}
      \Mat{W}
      \Vec{E}(\bar{\Vec{x}})
  }_{\Mat{A}}
  \underbrace{
    \vphantom{
      \Vec{E}(\bar{\Vec{x}})^{\transpose}
      \Mat{W}
      \Vec{E}(\bar{\Vec{x}})
    }
    \Delta\Vec{x}
  }_{\Vec{x}}
  =
  \underbrace{
    - \Vec{E}(\bar{\Vec{x}})^{\transpose}
    \Mat{W} \,
    \Vec{e}(\bar{\Vec{x}})
  }_{\Vec{b}}
</equation>
solve the normal equations for $\Delta\Vec{x}$ and update $\Vec{x}$ using,
<equation>
  \Vec{x}^{k+1} = \Vec{x}^{k} + \Delta{\Vec{x}}
</equation>
</p>


<hr><!------------------------------------------------------------------------!>


<h2>Gauss Newton</h2>

<equation>
  \min_{\Vec{x}} \cost(\Vec{x})
    &=
      \dfrac{1}{2}
      \sum_{i}
      \Vec{e}_{i}^{\transpose} \Mat{W} \Vec{e}_{i} \\
    &=
      \dfrac{1}{2} \enspace
      \Vec{e}_{i}^{\transpose}(\Vec{x})
      \Mat{W}
      \Vec{e}_{i}(\Vec{x})
</equation>

<p>Since the error function, $\Vec{e}(\Vec{x})$, is non-linear, it is
approximated with the first-order Taylor series,
<equation>
  \Vec{e}(\Vec{x})
    \approx
      \Vec{e}(\bar{\Vec{x}}) +
      \Mat{E}(\bar{\Vec{x}}) \Delta\Vec{x}
</equation>
where $\Mat{E}(\bar{\Vec{x}}) = \dfrac{\partial\Vec{e}(\Vec{x})}{\partial\Vec{x}}
\bigg\rvert_{\Vec{x}_{k}}$ and $\Delta{\Vec{x}} = \Vec{x} - \bar{\Vec{x}}$.</p>

<equation>
  \dfrac{\partial{\cost}}{\partial{\Vec{x}}} =
    \dfrac{\partial{\cost}}{\partial{\Vec{e}}}
    \dfrac{\partial{\Vec{e}}}{\partial{\Vec{x}}}
</equation>

<equation>

  \dfrac{\partial{\cost}}{\partial{\Vec{e}}} &=
    \dfrac{1}{2} \Vec{e}^{\transpose}(\Vec{x}) \Mat{W} \Vec{e}(\Vec{x}) =
    \Vec{e}^{\transpose}(\Vec{x}) \Mat{W} \\
  %
  \dfrac{\partial{\Vec{e}}}{\partial{\Vec{x}}} &=
    \Vec{e}(\bar{\Vec{x}}) +
    \Mat{E}(\bar{\Vec{x}}) \Delta\Vec{x} =
    \Mat{E}(\bar{\Vec{x}})

</equation>

<equation>

  \dfrac{\partial{\cost}}{\partial{\Vec{x}}}
    &=
      (\Vec{e}^{\transpose}(\Vec{x}) \Mat{W}) (\Mat{E}(\bar{\Vec{x}})) \\
    % Line 2
    &=
      (
        \Vec{e}(\bar{\Vec{x}}) + \Mat{E}(\bar{\Vec{x}}) \Delta\Vec{x}
      )^{\transpose} \Mat{W}
      \Mat{E}(\bar{\Vec{x}}) \\
    % Line 3
    &=
      \Vec{e}^{\transpose}(\bar{\Vec{x}}) \Mat{W} \Mat{E}(\bar{\Vec{x}})
      + \Delta\Vec{x}^{\transpose}
        \Mat{E}(\bar{\Vec{x}})^{\transpose} \Mat{W} \Mat{E}(\bar{\Vec{x}})
      = 0 \\

</equation>

<equation>
    % Line 4
    \Delta\Vec{x}^{\transpose}
      \Mat{E}(\bar{\Vec{x}})^{\transpose} \Mat{W} \Mat{E}(\bar{\Vec{x}})
    &=
      - \Vec{e}^{\transpose}(\bar{\Vec{x}}) \Mat{W} \Mat{E}(\bar{\Vec{x}}) \\
    % Line 5
    \underbrace{
      \Mat{E}(\bar{\Vec{x}})^{\transpose} \Mat{W} \Mat{E}(\bar{\Vec{x}})
    }_{\Mat{H}}
      \Delta\Vec{x}
    &=
    \underbrace{
      - \Mat{E}(\bar{\Vec{x}})^{\transpose} \Mat{W} \Vec{e}(\bar{\Vec{x}})
    }_{\Vec{b}}

</equation>

<p>Solve the normal equations $\Mat{H}\Delta\Vec{x} = \Vec{b}$ for
$\Delta\Vec{x}$ using the Cholesky or QR-decompositon. Once $\Delta\Vec{x}$ is
found the best estimate $\bar{\Vec{x}}$ can be updated via,
<equation>
  \bar{\Vec{x}}_{k + 1} = \bar{\Vec{x}}_{k} + \Delta\Vec{x}.
</equation>
</p>
