<h1>Non-linear Least Squares</h1>


<hr><!------------------------------------------------------------------------!>

<h2>Gauss Newton</h2>

<p>
<equation>
  \min_{\Vec{x}} \cost(\Vec{x})
    &=
      \dfrac{1}{2}
      \sum_{i}
      \Vec{e}_{i}^{\transpose} \Mat{W} \Vec{e}_{i} \\
    &=
      \dfrac{1}{2} \enspace
      \Vec{e}_{i}^{\transpose}(\Vec{x})
      \Mat{W}
      \Vec{e}_{i}(\Vec{x})
</equation>
where the error function, $\Vec{e}(\cdot)$, depends on the optimization
parameter, $\Vec{x} \in \real^{n}$. The error function, $\Vec{e}(\cdot)$, has a
form of
<equation>
  \Vec{e}_{i} =
    \Vec{z} - \Vec{h}(\Vec{x})
</equation>
is defined as the difference between the measured value, $\Vec{z}$, and the
estimated value calculated using the measurement function, $\Vec{h}(\cdot)$.
Since the error function, $\Vec{e}(\Vec{x})$, is non-linear, it is
approximated with the first-order Taylor series,
<equation>
  \Vec{e}(\Vec{x})
    \approx
      \Vec{e}(\bar{\Vec{x}}) +
      \Mat{E}(\bar{\Vec{x}}) \Delta\Vec{x}
</equation>
where $\Mat{E}(\bar{\Vec{x}}) = \dfrac{\partial\Vec{e}(\Vec{x})}{\partial\Vec{x}}
\bigg\rvert_{\Vec{x}_{k}}$ and $\Delta{\Vec{x}} = \Vec{x} - \bar{\Vec{x}}$.</p>

<equation>
  \dfrac{\partial{\cost}}{\partial{\Vec{x}}} =
    \dfrac{\partial{\cost}}{\partial{\Vec{e}}}
    \dfrac{\partial{\Vec{e}}}{\partial{\Vec{x}}}
</equation>

<equation>

  \dfrac{\partial{\cost}}{\partial{\Vec{e}}} &=
    \dfrac{1}{2} \Vec{e}^{\transpose}(\Vec{x}) \Mat{W} \Vec{e}(\Vec{x}) =
    \Vec{e}^{\transpose}(\Vec{x}) \Mat{W} \\
  %
  \dfrac{\partial{\Vec{e}}}{\partial{\Vec{x}}} &=
    \Vec{e}(\bar{\Vec{x}}) +
    \Mat{E}(\bar{\Vec{x}}) \Delta\Vec{x} =
    \Mat{E}(\bar{\Vec{x}})

</equation>

<equation>

  \dfrac{\partial{\cost}}{\partial{\Vec{x}}}
    &=
      (\Vec{e}^{\transpose}(\Vec{x}) \Mat{W}) (\Mat{E}(\bar{\Vec{x}})) \\
    % Line 2
    &=
      (
        \Vec{e}(\bar{\Vec{x}}) + \Mat{E}(\bar{\Vec{x}}) \Delta\Vec{x}
      )^{\transpose} \Mat{W}
      \Mat{E}(\bar{\Vec{x}}) \\
    % Line 3
    &=
      \Vec{e}^{\transpose}(\bar{\Vec{x}}) \Mat{W} \Mat{E}(\bar{\Vec{x}})
      + \Delta\Vec{x}^{\transpose}
        \Mat{E}(\bar{\Vec{x}})^{\transpose} \Mat{W} \Mat{E}(\bar{\Vec{x}})
      = 0 \\

</equation>

<equation>
    % Line 4
    \Delta\Vec{x}^{\transpose}
      \Mat{E}(\bar{\Vec{x}})^{\transpose} \Mat{W} \Mat{E}(\bar{\Vec{x}})
    &=
      - \Vec{e}^{\transpose}(\bar{\Vec{x}}) \Mat{W} \Mat{E}(\bar{\Vec{x}}) \\
    % Line 5
    \underbrace{
      \Mat{E}(\bar{\Vec{x}})^{\transpose} \Mat{W} \Mat{E}(\bar{\Vec{x}})
    }_{\Mat{H}}
      \Delta\Vec{x}
    &=
    \underbrace{
      - \Mat{E}(\bar{\Vec{x}})^{\transpose} \Mat{W} \Vec{e}(\bar{\Vec{x}})
    }_{\Vec{b}}

</equation>

<p>Solve the normal equations $\Mat{H}\Delta\Vec{x} = \Vec{b}$ for
$\Delta\Vec{x}$ using the Cholesky or QR-decompositon. Once $\Delta\Vec{x}$ is
found the best estimate $\bar{\Vec{x}}$ can be updated via,
<equation>
  \bar{\Vec{x}}_{k + 1} = \bar{\Vec{x}}_{k} + \Delta\Vec{x}.
</equation>
</p>
