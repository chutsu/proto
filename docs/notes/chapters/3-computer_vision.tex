\chapter{Computer Vision}

\section{Pinhole Camera Model}

\begin{equation}
  K =
  \begin{bmatrix}
    f_x & 0 & c_x \\
    0 & f_y & c_y \\
    0 & 0 & 1
  \end{bmatrix}
\end{equation}



\section{Radial Tangential Distortion}

\begin{align}
\begin{split}
  k_{\text{radial}} &= 1 + (k_1 r^2) + (k_2 r^4) \\
  x' &= x \cdot k_{\text{radial}} \\
  y' &= y \cdot k_{\text{radial}} \\
  x'' &= x' + (2 p_1 x y + p_2 (r^2 + 2 x^2)) \\
  y'' &= y' + (p_1 (r^2 + 2 y^2) + 2 p_2 x y)
\end{split}
\end{align}



\section{Equi-distant Distortion}

\begin{align}
\begin{split}
  r &= \sqrt{x^{2} + y^{2}} \\
  \theta &= \arctan{(r)} \\
  \theta_d &= \theta (1 + k_1 \theta^2 + k_2 \theta^4 + k_3 \theta^6 + k_4 \theta^8) \\
  x' &= (\theta_d / r) \cdot x \\
  y' &= (\theta_d / r) \cdot y
\end{split}
\end{align}



% FEATURE ESTIMATION
\section{Feature Estimation}
\label{sec:feature_estimation}

In the previous section we have discussed methods to detect and match between
different camera frames. In this section we introduce a method to estimate
feature positions in 3D using 2D pixels measurements of the same feature in
different camera frames.

\subsection{Feature Initialization}
\label{subsec:linear_triangulation}

There are various methods for initializing the feature position. The linear
triangulation method~\cite{Hartley2003} is frequently used. This method assumes
a pair of homogeneous pixel measurements $\measurement$ and $\measurement' \in
\real^{3}$ that observes the same feature, $\mathbf{X}$, in homogeneous
coordinates from two different camera frames. The homogeneous projection from
3D to 2D with a known camera matrix $\Mat{P} \in \real^{3 \times 4}$
for each measurement is given as,
%
\begin{align}
\begin{split}
	\measurement &= \mathbf{P} \mathbf{X} \\
	\measurement' &= \mathbf{P}' \mathbf{X}.
\end{split}
\end{align}
%
These equations can be combined to form a system of equations of the form
$\Mat{A} \Vec{x} = 0$. To eliminate the homogeneous scale factor we apply a
cross product to give three equations for each image point, for example
$\measurement \times (\Mat{P} \Mat{X}) = \Vec{0}$ writing this out gives
%
\begin{align}
\label{eq:linear_triangulation_derivation}
\begin{split}
  x (\Vec{p}^{3T} \Vec{X}) - (\Vec{p}^{1T} \Vec{X}) = 0 \\
  y (\Vec{p}^{3T} \Vec{X}) - (\Vec{p}^{2T} \Vec{X}) = 0 \\
  x (\Vec{p}^{2T} \Vec{X}) - y (\Vec{p}^{1T} \Vec{X}) = 0
\end{split}
\end{align}
%
where $\Vec{p}^{iT}$ is the $i^{\mbox{th}}$ row of $\Vec{P}$.

From Eq.~\eqref{eq:linear_triangulation_derivation}, an equation of the form
$\Mat{A} \Vec{x} = \Vec{0}$ for each image point can be formed, where
$\Vec{x}$ represents the unknown homogeneous feature location to be
estimated, and $\Mat{A}$ is given as
%
\begin{align}
  \mathbf{A} =
  \begin{bmatrix}
    x (\Vec{p}^{3T}) - (\Vec{p}^{1T}) \\
    y (\Vec{p}^{3T}) - (\Vec{p}^{2T}) \\
    x' (\Vec{p'}^{3T}) - (\Vec{p'}^{1T}) \\
    y' (\Vec{p'}^{3T}) - (\Vec{p'}^{2T})
  \end{bmatrix}
  \label{eq:linear_triangulation_ derivation}
\end{align}
%
giving a total of four equations in four homogeneous unknowns. Solving for
$\Vec{A}$ using SVD allows us to estimate the initial feature location.


\subsection{Feature Refinement}
\label{subsec:bundle_adjustment}

The linear triangulation provides an initial feature position. In an ideal
world, feature positions can be solved as a system of equations. In reality,
however, errors are present in the camera poses and pixel measurements. The
pixel measurements observing the same 3D point are generally noisy. In
addition, the pinhole camera model and radial-tangential distortion model do
not perfectly model the camera projection or distortion observed. Therefore an
iterative method can be used to further refine the feature position. This
problem is generally formulated as a non-linear least square problem and can be
solved by numerical methods, such as the Gauss-Newton algorithm.

Let us consider the case where two different cameras at known locations observe
the same feature in the scene. Our goal is to optimize for the feature
position. With an $i^{\text{th}}$ feature measurement, and corresponding
$i^{\text{th}}$ camera orientation as a quaternion, $\quat_{\cam_{i}\world}$ and
$i^{\text{th}}$ camera position, $\pos_{\cam_{i}}^{\cam_{i}\world}$, we can
formulate the re-projection error as,
%
\begin{equation}
	\label{eq:bundle_adjustment_residual}
	\mathbf{e}_{i}(\boldsymbol{\theta},
								 \quat_{\cam_{i}\world},
								 \pos_{\cam_{i}}^{\cam_{i}\world})
	=
	\measurement_{i}
	- \mathbf{h}(\boldsymbol{\theta},
							 \quat_{\cam_{i}\world},
							 \pos_{\cam_{i}}^{\cam_{i}\world}).
\end{equation}
%
where the parameter, $\boldsymbol{\theta} \in \real^{3}$, represents the
feature location to be optimized, $\measurement_{i} \in \real^{2}$ is the
$i$-th pixel measurement, and $\mathbf{h} : \real^{10} \mapsto \real^{2}$ is
the projection function that projects $\boldsymbol{\theta}$ into the
measurement space using known camera extrinsics. The re-projection error is a
geometric error corresponding to the Euclidean distance between measured and
projected features onto the image plane, it is used for quantifying the error
of the estimated feature location. With the error function, $\Vec{e}_{i}$,
for one measurement, a cost function, $\Vec{f}$, for $m$ number of pixel
measurements in the form of sum of squares can be defined as,
%
\begin{equation}
  \label{eq:sfm}
  \mathbf{f}(\boldsymbol{\theta})
	= \sum^{m}_{i=1}
    \mathbf{e}_{i}(
	    \boldsymbol{\theta},
	    \quat_{\cam_{i}\world},
	    \pos_{\cam_{i}}^{\cam_{i}\world}
	)^{T}
	\mathbf{e}_{i}(
		\boldsymbol{\theta},
		\quat_{\cam_{i}\world},
		\pos_{\cam_{i}}^{\cam_{i}\world}
	).
\end{equation}
%
Now that the cost function, $\mathbf{f}(\boldsymbol{\theta})$, is defined an
unconstrained optimization can be performed using the cost function
Eq.~\ref{eq:sfm} to estimate the optimal feature position,
$\boldsymbol{\theta}^{*}$, which minimizes the re-projection error over the set
of collected measurements.
%
\begin{equation}
	\boldsymbol{\theta}^{*} = \argmin{\theta} \mathbf{f}(\boldsymbol{\theta}).
\end{equation}
%
%Note, to be clear, $\theta$ is usually the vector of all estimated parameters.
%This includes the camera pose, feature locations, and can also include
%calibration parameters.



\section{Bundle Adjustment}

Let $\measurement \in \re^{2}$ be the image measurement and $\projFunc(\cdot)
\in \re^{2}$ be the projection function that produces an image projection
$\estimate$. The reprojection error $e$ is defined as the euclidean distance
between $\measurement$ and $\estimate$.

\begin{equation}
  \error = \measurement - \estimate
\end{equation}

Our aim given image measurement $\measurement$ is to find the image projection
$\estimate$ that minimizes the reprojection $e$. The image projection
$\estimate$ in pixels can be represented in homogeneous coordinates with
$u, v, w$ as
%
\begin{equation}
  \estimate
  = \begin{bmatrix} x \\ y \end{bmatrix}
  = \begin{bmatrix} u / w \\ v / w \end{bmatrix}
\end{equation}
%
% Projection function
\begin{align}
  \label{eq:projection_function}
  \begin{bmatrix} u \\ v \\ w \end{bmatrix}
    = \Mat{P} \begin{bmatrix} \Pt{\world} \\ 1 \end{bmatrix}
      = \Mat{K} \camRot [\Pt{\world} - \camPos]
\end{align}
%
where $u, v, w$ is computed by projecting a landmark position $\Pt{\world}$ in
the world frame to the camera's image plane with projection matrix $\Mat{P}$.
The projection matrix, $\Mat{P}$, can be decomposed into the camera intrinsics
matrix, $\Mat{K}$, the camera rotation, $\camRot$, and camera position,
$\camPos$, expressed in the world frame.
%
The orientation of the camera in \eqref{eq:projection_function} is
represented using a rotation matrix. To reduce the optimization parameters
the rotation matrix can be parameterized by a quaternion by using the following
formula,
%
\begin{equation}
  \camRot = \begin{bmatrix}
    % Line 1
    q_{w}^{2} + q_{x}^{2} - q_{y}^{2} - q_{z}^{2}
    & 2 (q_{x} q_{y} - q_{w} q_{z})
    & 2 (q_{x} q_{z} + q_{w} q_{y}) \\
    % Line 2
    2 (q_{x} q_{y} + q_{w} q_{z})
    & q_{w}^{2} - q_{x}^{2} + q_{y}^{2} - q_{z}^{2}
    & 2 (q_{y} q_{z} - q_{w} q_{x}) \\
    % Line 3
    2 (q_{x} q_{z} - q_{w} q_{y})
    & 2 (q_{y} q_{z} + q_{w} q_{x})
    & q_{w}^{2} - q_{x}^{2} - q_{y}^{2} + q_{z}^{2}
  \end{bmatrix}.
\end{equation}
%
By parameterzing the rotation matrix with a quaternion, the optimization
parameters for the camera's orientation is reduced from 9 to 4.

Our objective is to optimize for the camera rotation $\camRot$, camera
position $\camPos$ and 3D landmark position $\Pt{\world}$ in order to
minimize the cost function,
%
\begin{align}
  &\Argmin{\camRot, \camPos, \Pt{\world}} \Norm{
    \measurement - \projFunc(\camRot, \camPos, \Pt{\world})
  }^{2} \\
  &\Argmin{\camRot, \camPos, \Pt{\world}} \Norm{
    \begin{bmatrix} x \\ y \end{bmatrix} -
    \begin{bmatrix} u / w \\ v / w \end{bmatrix}
    }^{2}.
\end{align}
%
The cost function above assumes only a single measurement, if there are $N$
measurements corresponding to $N$ unique landmarks the cost function can be
rewritten as a maximum likelihood estimation problem as,
%
\begin{equation}
  \Argmin{\camRot, \camPos, \Pt{\world}}
  \sum_{j = 1}^{N}
  \Norm{
    \measurement_{j} - \projFunc(\camRot^{j}, \camPos^{j}, \Point{\world}{j})
  }^{2}
\end{equation}
%
under the assumption that the observed landmark, $\Pt{\world}$, measured in
the image plane, $z$, are corrupted by a \textbf{zero-mean Gaussian noise}.

For the general case of $M$ images taken at different camera poses the cost
function can be further extended to,
%
\begin{equation}
  \min_{\camRot, \camPos, \Pt{\world}} 
  \sum_{i = 1}^{M} \sum_{j = 1}^{N}
  \Norm{
    \measurement_{i, j}
    - \projFunc(\camRot^{i}, \camPos^{i}, \Point{\world}{j})
  }^{2}
\end{equation}
%
The optimization process begins by setting the first image camera pose as world
origin, and subsequent $\camRot_{i}$ and $\camPos_{i}$ will be relative to the
first camera pose.


\subsection*{Jacobians}

The Jacobian for the optimization problem for a \textbf{single measurement} has
the form:
%
\begin{equation}
  \jac = \begin{bmatrix}
    \color{red}
    \dfrac{\partial{\projFunc}}{\partial \camRot}
    \color{cyan}
    \dfrac{\partial{\camRot}}{\partial{\quat}} \quad
    \color{Mulberry}
    \dfrac{\partial{\projFunc}}{\partial \camPos} \quad
    \color{blue}
    \dfrac{\partial{\projFunc}}{\partial \Pt{\world}}
  \end{bmatrix} \\
\end{equation}
%
If there are two measurements the Jacobian is stacked with the following
pattern:
%
\begin{equation}
  \jac = \begin{bmatrix}
    \text{Image 1}_{2 \times 7}
      & \Zeros{2}{7}
      & \text{3D Point}_{2 \times 3} \\
    \Zeros{2}{7}
      & \text{Image 2}_{2 \times 7}
      & \text{3D Point}_{2 \times 3}
  \end{bmatrix}
\end{equation}


% DERIVATION FOR dh / dR
\subsubsection*{Derivation for
$\color{red}
\dfrac{\partial{\projFunc}}{\partial{\camRot}}$}

\begin{align}
  \color{red}
  \dfrac{\partial{\projFunc}}{\partial{\camRot}} =
  \begin{bmatrix}
    \dfrac{
      w \dfrac{\partial{u}}{\partial{\camRot}} -
      u \dfrac{\partial{w}}{\partial{\camRot}}
    }{w^{2}} \vspace{1.0em} \\
    \dfrac{
      w \dfrac{\partial{v}}{\partial{\camRot}} -
      v \dfrac{\partial{w}}{\partial{\camRot}}
    }{w^{2}} \vspace{0.5em}
  \end{bmatrix}_{2 \times 9}
\end{align}

\begin{align}
  % Line 1
  \begin{bmatrix} u \\ v \\ w \end{bmatrix}
    &= \Mat{K} \camRot [\Pt{\world} - \camPos]
    \nonumber \\
  % Line 2
  \begin{bmatrix} u \\ v \\ w \end{bmatrix}
  &= \begin{bmatrix}
      f_x & 0 & p_x \\
      0 & f_y & p_y \\
      0 & 0 & 1
  \end{bmatrix}
  \begin{bmatrix}
      R_{11} & R_{12} & R_{13} \\
      R_{21} & R_{22} & R_{23} \\
      R_{31} & R_{32} & R_{33}
  \end{bmatrix}
  [\Pt{\world} - \camPos]
  \nonumber
\end{align}

\begin{align}
  \dfrac{\partial{u}}{\partial{\camRot}} &=
      \begin{bmatrix}
        f_{x} (\Pt{\world} - \camPos) &
        \Zeros{1}{3} &
        p_{x} (\Pt{\world} - \camPos)
      \end{bmatrix}_{1 \times 9} \\
  \dfrac{\partial{v}}{\partial{\camRot}} &=
    \begin{bmatrix}
      \Zeros{1}{3} &
      f_{y} (\Pt{\world} - \camPos) &
      p_{y} (\Pt{\world} - \camPos)
  \end{bmatrix}_{1 \times 9} \\
  \dfrac{\partial{w}}{\partial{\camRot}} &=
    \begin{bmatrix}
      \Zeros{1}{3} &
      \Zeros{1}{3} &
      (\Pt{\world} - \camPos)
    \end{bmatrix}_{1 \times 9}
\end{align}


% DERIVATION FOR dR / dq
\subsubsection*{Derivation for \color{cyan}
$\dfrac{\partial{\camRot}}{\partial{\quat}}$}

\begin{align}
  \color{cyan}
  \dfrac{\partial{\camRot}}{\partial{\quat}} =
  \begin{bmatrix}
    \dfrac{\partial{\camRot_{11}}}{\partial{\quat}} \\
    \dfrac{\partial{\camRot_{12}}}{\partial{\quat}} \\
    \vdots \\
    \dfrac{\partial{\camRot_{33}}}{\partial{\quat}}
  \end{bmatrix}_{9 \times 4}
\end{align}

\begin{equation}
  \camRot = \begin{bmatrix}
    % Line 1
    q_{w}^{2} + q_{x}^{2} - q_{y}^{2} - q_{z}^{2}
    & 2 (q_{x} q_{y} - q_{w} q_{z})
    & 2 (q_{x} q_{z} + q_{w} q_{y}) \\
    % Line 2
    2 (q_{x} q_{y} + q_{w} q_{z})
    & q_{w}^{2} - q_{x}^{2} + q_{y}^{2} - q_{z}^{2}
    & 2 (q_{y} q_{z} - q_{w} q_{x}) \\
    % Line 3
    2 (q_{x} q_{z} - q_{w} q_{y})
    & 2 (q_{y} q_{z} + q_{w} q_{x})
    & q_{w}^{2} - q_{x}^{2} - q_{y}^{2} + q_{z}^{2}
  \end{bmatrix}
  \nonumber
\end{equation}

\begin{align}
  % R11
  \dfrac{\camRot_{11}}{\partial{\quat}} &=
    \begin{bmatrix}
    0 & -4q_{y} & -4q_{z} & 0
    \end{bmatrix} \\
  % R12
  \dfrac{\camRot_{12}}{\partial{\quat}} &=
    \begin{bmatrix}
      2q_{y} & 2q_{x} & -2q_{w} & -2q_{z}
    \end{bmatrix} \\
  % R13
  \dfrac{\camRot_{13}}{\partial{\quat}} &=
    \begin{bmatrix}
      2q_{z} & 2q_{w} & 2q_{x} & 2q_{y}
    \end{bmatrix} \\
  % R21
  \dfrac{\camRot_{21}}{\partial{\quat}} &=
  \begin{bmatrix}
    2q_{y} & 2q_{x} & 2q_{w} & 2q_{z}
  \end{bmatrix} \\
  % R22
  \dfrac{\camRot_{22}}{\partial{\quat}} &=
    \begin{bmatrix}
      4q_{x} & 0 & 4q_{z} & 0
    \end{bmatrix} \\
  % R23
  \dfrac{\camRot_{23}}{\partial{\quat}} &=
    \begin{bmatrix}
      2q_{w} & 2q_{z} & 2q_{y} & 2q_{x}
    \end{bmatrix} \\
  % R31
  \dfrac{\camRot_{31}}{\partial{\quat}} &=
    \begin{bmatrix}
      2q_{z} & -2q_{w} & 2q_{x} & -2q_{y}
    \end{bmatrix} \\
  % R32
  \dfrac{\camRot_{32}}{\partial{\quat}} &=
    \begin{bmatrix}
      -4q_{x} & -4q_{y} & 0 & 0
    \end{bmatrix} \\
  % R33
  \dfrac{\camRot_{33}}{\partial{\quat}} &=
    \begin{bmatrix}
      2q_{w} & 2q_{z} & 2q_{y} & 2q_{x}
    \end{bmatrix}
\end{align}


% DERIVATION FOR dh / dL
\subsubsection*{Derivation for
$\color{blue} \dfrac{\partial{\projFunc}}{\partial{\Pt{\world}}}$}

\begin{align}
  \color{blue}
  \dfrac{\partial{\projFunc}}{\partial{\Pt{\world}}} =
  \begin{bmatrix}
    \dfrac{
      w \dfrac{\partial{u}}{\partial{\Pt{\world}}} -
      u \dfrac{\partial{w}}{\partial{\Pt{\world}}}
    }{w^{2}} \vspace{1.0em} \\
    \dfrac{
      w \dfrac{\partial{v}}{\partial{\Pt{\world}}} -
      v \dfrac{\partial{w}}{\partial{\Pt{\world}}}
    }{w^{2}} \vspace{0.5em}
  \end{bmatrix}_{2 \times 3}
\end{align}

\begin{align}
  % Line 1
  \begin{bmatrix} u \\ v \\ w \end{bmatrix}
    &= \Mat{K} \camRot [\Pt{\world} - \camPos]
    \nonumber \\
  % Line 2
  \begin{bmatrix} u \\ v \\ w \end{bmatrix}
  &= \begin{bmatrix}
      f_x & 0 & p_x \\
      0 & f_y & p_y \\
      0 & 0 & 1
  \end{bmatrix}
  \begin{bmatrix}
      R_{11} & R_{12} & R_{13} \\
      R_{21} & R_{22} & R_{23} \\
      R_{31} & R_{32} & R_{33}
  \end{bmatrix}
  [\Pt{\world} - \camPos]
  \nonumber
\end{align}

\begin{align}
  \dfrac{\partial{u}}{\partial{\Pt{\world}}} &=
    \begin{bmatrix}
      f_{x} \camRot_{11} + c_{x} \camRot_{31} &
      f_{x} \camRot_{12} + c_{x} \camRot_{32} &
      f_{x} \camRot_{13} + c_{x} \camRot_{33}
    \end{bmatrix} \\
  \dfrac{\partial{v}}{\partial{\Pt{\world}}} &=
    \begin{bmatrix}
      f_{y} \camRot_{21} + c_{y} \camRot_{31} &
      f_{y} \camRot_{22} + c_{y} \camRot_{32} &
      f_{y} \camRot_{23} + c_{y} \camRot_{33}
    \end{bmatrix} \\
  \dfrac{\partial{w}}{\partial{\Pt{\world}}} &=
    \begin{bmatrix}
      \camRot_{31} &
      \camRot_{32} &
      \camRot_{33}
    \end{bmatrix}
\end{align}


% DERIVATION FOR dh / dC
\subsubsection*{Derivation for \color{Mulberry}
$\dfrac{\partial{h}}{\partial{\camPos}}$}

% Partial derivative of projection w.r.t camera position
\begin{align}
  \color{Mulberry}
  \dfrac{\partial{\projFunc}}{\partial{\camPos}} =
  \begin{bmatrix}
    \dfrac{
      w \dfrac{\partial{u}}{\partial{\camPos}} -
      u \dfrac{\partial{w}}{\partial{\camPos}}
    }{w^{2}} \vspace{1.0em} \\
    \dfrac{
      w \dfrac{\partial{v}}{\partial{\camPos}} -
      v \dfrac{\partial{w}}{\partial{\camPos}}
    }{w^{2}} \vspace{0.5em}
  \end{bmatrix}_{2 \times 3}
\end{align}

\begin{align}
  % Line 1
  \begin{bmatrix} u \\ v \\ w \end{bmatrix}
    &= \Mat{K} \camRot [\Pt{\world} - \camPos]
    \nonumber \\
  % Line 2
  \begin{bmatrix} u \\ v \\ w \end{bmatrix}
  &= \begin{bmatrix}
      f_x & 0 & p_x \\
      0 & f_y & p_y \\
      0 & 0 & 1
  \end{bmatrix}
  \begin{bmatrix}
      R_{11} & R_{12} & R_{13} \\
      R_{21} & R_{22} & R_{23} \\
      R_{31} & R_{32} & R_{33}
  \end{bmatrix}
  [\Pt{\world} - \camPos]
  \nonumber
\end{align}

\begin{align}
  \dfrac{\partial{u}}{\partial{\camPos}} &=
    -\begin{bmatrix}
      f_{x} \camRot_{11} + c_{x} \camRot_{31} &
      f_{x} \camRot_{12} + c_{x} \camRot_{32} &
      f_{x} \camRot_{13} + c_{x} \camRot_{33}
    \end{bmatrix} \\
  \dfrac{\partial{v}}{\partial{\camPos}} &=
    -\begin{bmatrix}
      f_{y} \camRot_{21} + c_{y} \camRot_{31} &
      f_{y} \camRot_{22} + c_{y} \camRot_{32} &
      f_{y} \camRot_{23} + c_{y} \camRot_{33}
    \end{bmatrix} \\
  \dfrac{\partial{w}}{\partial{\camPos}} &=
    -\begin{bmatrix}
      \camRot_{31} &
      \camRot_{32} &
      \camRot_{33}
    \end{bmatrix}
\end{align}



