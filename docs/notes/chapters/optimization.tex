\chapter{Optimization}

\section{Linear Least Squares}

Linear problems generally have the form
%
\begin{equation}
  \Mat{A} \Vec{x} = \Vec{b}
\end{equation}
%
If $\Mat{A}$ is skinny (number of rows is larger than number of columns) the
problem is over constrained and there is no \textit{unique} solution. Instead,
the problem can be solved by minizming the squared error between $\Mat{A}
\Vec{x}$ and $\Vec{b}$. The linear least squares problem is then defined as,
%
\begin{equation}
  &\min_{\Vec{x}} || \Mat{A} \Vec{x} - \Vec{b} ||^{2}_{2} \enspace,
\end{equation}
%
where the goal is to find an \textit{approximate} solution.

The local minima can be found when the derivative of the squared error is zero.
First the squared error is expanded to give:
%
\begin{align}
  & (\Mat{A} \Vec{x} - \Vec{b})^{\transpose}
    (\Mat{A} \Vec{x} - \Vec{b}) \\
  & (\Transpose{\Vec{x}} \Transpose{\Mat{A}} \Mat{A} \Vec{x}
    - 2 \Transpose{\Vec{b}} \Mat{A} \Vec{x}
    + \Transpose{\Vec{b}} \Vec{b})
\end{align}
%
then by differentiating the expanded squared error with respect to $\Vec{x}$,
setting the derivative to zero, and rearranging the equation with respect to
$\Vec{x}$ gives the following
%
\begin{align}
  % Line 1
  2 \Transpose{\Vec{x}} \Transpose{\Mat{A}} \Mat{A}
    - 2 \Transpose{\Vec{b}} \Mat{A} &= 0 \\
  % Line 2
  \Transpose{\Vec{x}} \Transpose{\Mat{A}} \Mat{A}
    &= \Transpose{\Vec{b}} \Mat{A} \\
  % Line 3
  \Transpose{\Mat{A}} \Mat{A} \Vec{x}
    &= \Transpose{\Mat{A}} \Vec{b} \\
  % Line 4
  \Vec{x}
    &= \left( \Transpose{\Mat{A}} \Mat{A} \right)^{-1}
      \Transpose{\Mat{A}} \Vec{b} \\
  % Line 5
  \Vec{x}
    &= \Mat{A}^{\dagger} \Vec{b} \enspace,
\end{align}
%
where $\left( \Transpose{\Mat{A}} \Mat{A} \right)^{-1} \Transpose{\Mat{A}}$ is
known as the pseudo inverse $\Mat{A}^{\dagger}$.



\section{Non-linear Least Squares}

A non-linear least squares is an optimization problem that minimizes the sum of
squares of the residual functions, and has the form
%
\begin{equation}
  \min_{\Vec{x}} \enspace
    \dfrac{1}{2}
    \Vec{e}(\Vec{x})^{\transpose}
    \Mat{W} \,
    \Vec{e}(\Vec{x})
\end{equation}
%
where the error function, $\Vec{e}(\cdot)$, depends on the optimization
parameter, $\Vec{x} \in \real^{n}$. The error function, $\Vec{e}(\cdot)$, has a
form of
%
\begin{equation}
  \Vec{e}_{i} =
    \Vec{z} - \Vec{h}(\Vec{x})
\end{equation}
%
is defined as the difference between the measured value, $\Vec{z}$, and the
estimated value calculated using the measurement function, $\Vec{h}(\cdot)$.

A local minima for the problem is found when the gradient of the cost,
$\Vec{C}$, is zero
%
\begin{align}
  \dfrac{\partial{\Vec{C}}}{\partial{\Vec{x}}}
  &=
    \dfrac{\partial{\Vec{C}}}{\partial{\Vec{e}}}
    \dfrac{\partial{\Vec{e}}}{\partial{\Vec{x}}} \\
  &=
    \Vec{e}(\Vec{x})^{\transpose}
    \Mat{W}
    \dfrac{\partial{\Vec{e}}}{\partial{\Vec{x}}} \\
  &=
    \Vec{e}(\Vec{x})^{\transpose}
    \Mat{W}
    \Vec{E}(\Vec{x})
\end{align}
%
linearizing $\Vec{e}(\Vec{x})$ with the first-order Taylor series,
$\Vec{e}(\Vec{x}) \approx \Vec{e}(\bar{\Vec{x}}) + \Vec{E}(\bar{\Vec{x}})
\Delta\Vec{x}$, gives,
%
\begin{align}
  \dfrac{\partial{\Vec{C}}}{\partial{\Vec{x}}}
  &=
    (\Vec{e}(\bar{\Vec{x}}) + \Vec{E}(\bar{\Vec{x}})\Delta\Vec{x})^{\transpose}
    \Mat{W} \Vec{E}(\Vec{x})
  = 0 \\
  &\Vec{e}(\bar{\Vec{x}})^{\transpose} \Mat{W} \Vec{E}(\bar{\Vec{x}})
    + \Delta\Vec{x}^{\transpose} \Vec{E}(\bar{\Vec{x}})^{\transpose}
      \Mat{W}
      \Vec{E}(\bar{\Vec{x}})
    = 0 \\
  &\Vec{E}(\bar{\Vec{x}})^{\transpose} \Mat{W} \, \Vec{e}(\bar{\Vec{x}})
    + \Vec{E}(\bar{\Vec{x}})^{\transpose}
      \Mat{W}
      \Vec{E}(\bar{\Vec{x}})
      \Delta\Vec{x}
    = 0 \\
  &\underbrace{
      \Vec{E}(\bar{\Vec{x}})^{\transpose}
      \Mat{W}
      \Vec{E}(\bar{\Vec{x}})
  }_{\Mat{A}}
  \underbrace{
    \vphantom{
      \Vec{E}(\bar{\Vec{x}})^{\transpose}
      \Mat{W}
      \Vec{E}(\bar{\Vec{x}})
    }
    \Delta\Vec{x}
  }_{\Vec{x}}
  =
  \underbrace{
    - \Vec{E}(\bar{\Vec{x}})^{\transpose}
    \Mat{W} \,
    \Vec{e}(\bar{\Vec{x}})
  }_{\Vec{b}}
\end{align}
%
solve the normal equations for $\Delta\Vec{x}$ and update $\Vec{x}$ using,
%
\begin{equation}
  \Vec{x}^{k+1} = \Vec{x}^{k} + \Delta{\Vec{x}}
\end{equation}



\section{Gauge}

Gauge theory is borrowed from physics.

Accurate structure from motion or vision based state estimation is hard. One
hurdle is addressing the accuracy quantitatively. There are two main problems
that arise:

\begin{itemize}
  \item{\textbf{Inherent Physical Indeterminancy}: cause by loss of information
    while projecting 3D objects onto a 2D image plane.}
  \item{\textbf{Overparameterized Problem}: e.g. a shape model that can be
    parameterized by a vector, each representing the absolute position and
    orientation of the object could itself be indeterminant.}
\end{itemize}

It is well known that a vision only bundle adjustment has 7 unobserable
degrees-of-freedom (DoF), while for a VI-system, the global position and global
yaw is not observable, a total of four unobservable DoFs. These unobservable
DoFs (a.k.a gauge freedoms) have to be handled properly.

There are three main approaches to address the unobservability in a VI-system.
They are \textit{gauge fixation}, \textit{gauge prior}, \textit{free gauge}.


\subsection{Gauge fixation}

Gauge fixation method works by decreasing the number of optimization parameters to
where there are no unobservable states left for the opitmization problem to
optimize. This is to ensure the Hessian is well conditioned and invertable.
This approach enforces hard constraints to the solution.

The standard method to update orientation variables such as a rotation,
$\rot$, during the iterations of a non-linear least squares solver is to use
local coordinates, where at the $k$-th iteration, the update is
%
\begin{equation}
  \label{eq:opt-rot_std_update}
  \rot^{k + 1} = \text{Exp}(\delta \boldsymbol{\phi}^{k}) \rot^{k} .
\end{equation}

Setting the $z$ component of $\boldsymbol{\phi}^{k}$ to 0 allows fixating the
yaw with respect to $\rot^{k}$. However, concatenating several such updates
over $K$-iterations,
%
\begin{equation}
  \rot^{K} = \prod^{K-1}_{k=0} \text{Exp}(\delta \boldsymbol{\phi}^{k}) ,
\end{equation}
%
does not fixate the yaw with respect to the initial rotation $\rot^{0}$, and
therefore, this parameterization cannot be used to fix the yaw-value of
$\rot^{K}$ to that of the initial value $\rot^{0}$.

Although pose fixation or prior can be applied to any camera pose, it is common
practice to fixate the first camera.
%
\begin{equation}
  \pos_{0} = \pos^{0}_{0} ,
  \enspace
  \Delta \boldsymbol{\phi}_{0 z}
    \dot{=} \, \Vec{e}^{\transpose}_{z} \boldsymbol{\phi}_{0} = 0 \, ,
\end{equation}
%
where $\pos^{0}_{0}$ is the initial position of the first camera. Which is
equivalent to setting the corresponding columns of the Jacobian of the residual
vector to zero, namely $\jac_{\pos_0} = 0$, $\jac_{\Delta \phi_{0 z}} = 0$.
Thus, for rotations of the other camera poses, the standard iterative update
Eq.~\eqref{eq:opt-rot_std_update} is used, and, for the first camera rotation,
$\rot_{0}$, a more convenient parameterization is used. Instead of directly
using $\rot_{0}$, a left-multiplicative increment is used.
%
\begin{equation}
  \rot_{0} = \text{Exp}(\Delta \boldsymbol{\phi}_{0}) \rot^{0}_{0} \, ,
\end{equation}
%
where the rotation vector $\Delta \boldsymbol{\phi}_{0}$ is initialized to zero
and updated.


\subsection{Gauge prior}

Gauge prior augments the objective function with an additional penalty to favor
a solution that satisfies certain constraints in a soft manner.
%
\begin{equation}
  \Norm{\Vec{e}^{\pos}_{0}}^{2}_{\Sigma^{\pos}_{0}} \, ,
  \quad \text{where} \quad
  \Vec{e}^{\pos}_{0}(\boldsymbol{\theta})
    \enspace \dot{=} \enspace
    (\pos_{0} - \pos^{0}_{0}, \enspace \Delta \phi_{0 z})
\end{equation}



\subsection{Free gauge}

Free gauge is the most general, lets the optimization parameters evolve freely.
In order to deal with the singularity with the Hessian, the pseudo inverse is
used or some preconditioning method inorder to make the Hessian
well-conditioned and invertible.
